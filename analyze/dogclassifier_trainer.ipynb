{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    folder = root.find('folder').text\n",
    "    filename = root.find('filename').text\n",
    "    label = root.find('object').find('name').text\n",
    "    \n",
    "    bndbox = root.find('object').find('bndbox')\n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "    \n",
    "    return folder, filename, label, (xmin, ymin, xmax, ymax)\n",
    "\n",
    "class DogBreedDataset(Dataset):\n",
    "    def __init__(self, annotations_dir, images_dir, label_map, transform=None):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for subfolder in os.listdir(self.annotations_dir):\n",
    "            for xml_file in os.listdir(os.path.join(self.annotations_dir, subfolder)):\n",
    "                xml_path = os.path.join(self.annotations_dir, subfolder, xml_file)\n",
    "                folder, filename, label, bbox = parse_annotation(xml_path)\n",
    "                img_path = os.path.join(self.images_dir, subfolder, xml_file)\n",
    "                label_idx = self.label_map[label]\n",
    "                data.append((img_path, label_idx, bbox))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_idx, bbox = self.data[idx]\n",
    "        img_path = f\"{img_path}.jpg\"\n",
    "        \n",
    "        # Detect image format and open image\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image = Image.open(f).convert(\"RGB\")\n",
    "        \n",
    "        # Crop the image using the bounding box\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        image = image.crop((xmin, ymin, xmax, ymax))\n",
    "    \n",
    "        # One-hot encode the label\n",
    "        label = torch.zeros(len(self.label_map))\n",
    "        label[label_idx] = 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 120  # Replace with the actual number of dog breeds\n",
    "data_folder = \"C:\\\\bccn\\programming\\data\"\n",
    "images_dir = os.path.join(data_folder, 'images')\n",
    "annotations_dir = os.path.join(data_folder, 'Annotation')\n",
    "\n",
    "all_labels = []\n",
    "for subfolder in os.listdir(annotations_dir):\n",
    "    all_labels.append(subfolder[10:])\n",
    "\n",
    "label_map = {breed: idx for idx, breed in enumerate(all_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = DogBreedDataset(annotations_dir, images_dir, label_map, transform=transform)\n",
    "\n",
    "#create full dataloader\n",
    "# dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "#train test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "#create train and test dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\programming\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\programming\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "conv1.weight: False\n",
      "bn1.weight: False\n",
      "bn1.bias: False\n",
      "layer1.0.conv1.weight: True\n",
      "layer1.0.bn1.weight: True\n",
      "layer1.0.bn1.bias: True\n",
      "layer1.0.conv2.weight: True\n",
      "layer1.0.bn2.weight: True\n",
      "layer1.0.bn2.bias: True\n",
      "layer1.1.conv1.weight: False\n",
      "layer1.1.bn1.weight: False\n",
      "layer1.1.bn1.bias: False\n",
      "layer1.1.conv2.weight: False\n",
      "layer1.1.bn2.weight: False\n",
      "layer1.1.bn2.bias: False\n",
      "layer2.0.conv1.weight: False\n",
      "layer2.0.bn1.weight: False\n",
      "layer2.0.bn1.bias: False\n",
      "layer2.0.conv2.weight: False\n",
      "layer2.0.bn2.weight: False\n",
      "layer2.0.bn2.bias: False\n",
      "layer2.0.downsample.0.weight: False\n",
      "layer2.0.downsample.1.weight: False\n",
      "layer2.0.downsample.1.bias: False\n",
      "layer2.1.conv1.weight: False\n",
      "layer2.1.bn1.weight: False\n",
      "layer2.1.bn1.bias: False\n",
      "layer2.1.conv2.weight: False\n",
      "layer2.1.bn2.weight: False\n",
      "layer2.1.bn2.bias: False\n",
      "layer3.0.conv1.weight: False\n",
      "layer3.0.bn1.weight: False\n",
      "layer3.0.bn1.bias: False\n",
      "layer3.0.conv2.weight: False\n",
      "layer3.0.bn2.weight: False\n",
      "layer3.0.bn2.bias: False\n",
      "layer3.0.downsample.0.weight: False\n",
      "layer3.0.downsample.1.weight: False\n",
      "layer3.0.downsample.1.bias: False\n",
      "layer3.1.conv1.weight: False\n",
      "layer3.1.bn1.weight: False\n",
      "layer3.1.bn1.bias: False\n",
      "layer3.1.conv2.weight: False\n",
      "layer3.1.bn2.weight: False\n",
      "layer3.1.bn2.bias: False\n",
      "layer4.0.conv1.weight: False\n",
      "layer4.0.bn1.weight: False\n",
      "layer4.0.bn1.bias: False\n",
      "layer4.0.conv2.weight: False\n",
      "layer4.0.bn2.weight: False\n",
      "layer4.0.bn2.bias: False\n",
      "layer4.0.downsample.0.weight: False\n",
      "layer4.0.downsample.1.weight: False\n",
      "layer4.0.downsample.1.bias: False\n",
      "layer4.1.conv1.weight: True\n",
      "layer4.1.bn1.weight: True\n",
      "layer4.1.bn1.bias: True\n",
      "layer4.1.conv2.weight: True\n",
      "layer4.1.bn2.weight: True\n",
      "layer4.1.bn2.bias: True\n",
      "fc.weight: True\n",
      "fc.bias: True\n"
     ]
    }
   ],
   "source": [
    "#transer learning model import\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except the first and last\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer1.0' not in name and 'fc' not in name and 'layer4.1' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "#print model layers\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.5184\n",
      "Test loss: 2.6461\n",
      "Epoch [2/20], Loss: 1.7966\n",
      "Test loss: 2.3254\n",
      "Epoch [3/20], Loss: 1.8030\n",
      "Test loss: 2.3988\n",
      "Epoch [4/20], Loss: 1.7790\n",
      "Test loss: 1.9414\n",
      "Epoch [5/20], Loss: 1.7773\n",
      "Test loss: 1.9992\n",
      "Epoch [6/20], Loss: 1.7598\n",
      "Test loss: 2.4329\n",
      "Epoch [7/20], Loss: 1.7579\n",
      "Test loss: 2.3419\n",
      "Epoch [8/20], Loss: 1.7308\n",
      "Test loss: 2.1174\n",
      "Epoch [9/20], Loss: 1.7316\n",
      "Test loss: 2.1535\n",
      "Epoch [10/20], Loss: 1.7113\n",
      "Test loss: 1.9253\n",
      "Epoch [11/20], Loss: 1.6779\n",
      "Test loss: 1.9509\n",
      "Epoch [12/20], Loss: 1.6807\n",
      "Test loss: 1.8811\n",
      "Epoch [13/20], Loss: 1.6701\n",
      "Test loss: 2.2138\n",
      "Epoch [14/20], Loss: 1.6454\n",
      "Test loss: 1.9070\n",
      "Epoch [15/20], Loss: 1.6521\n",
      "Test loss: 2.1030\n",
      "Epoch [16/20], Loss: 1.6337\n",
      "Test loss: 1.8079\n",
      "Epoch [17/20], Loss: 1.6288\n",
      "Test loss: 2.0362\n",
      "Epoch [18/20], Loss: 1.6333\n",
      "Test loss: 2.8483\n",
      "Epoch [19/20], Loss: 1.6101\n",
      "Test loss: 2.4791\n",
      "Epoch [20/20], Loss: 1.6109\n",
      "Test loss: 3.3296\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "#regularized cross entropy loss and adam optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#l2 regularization\n",
    "l2_lambda = 0.005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=l2_lambda)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 20  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Test loss: {running_loss/len(test_dataloader):.4f}\")\n",
    "\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(model, 'dog_breed_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bccn_classes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
